{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import gensim\n",
    "\n",
    "import scikitplot as skplt\n",
    "\n",
    "import nltk\n",
    "\n",
    "import os\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Bidirectional\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need pandas dataframe sample_train\n",
    "\n",
    "def evaluate_features(X, y, clf=None):\n",
    "\n",
    "    if clf is None:\n",
    "        clf = LogisticRegression()\n",
    "    \n",
    "    probas = cross_val_predict(clf, X, y, cv=StratifiedKFold(random_state=8), \n",
    "                              n_jobs=-1, method='predict_proba', verbose=2)\n",
    "    pred_indices = np.argmax(probas, axis=1)\n",
    "    classes = np.unique(y)\n",
    "    preds = classes[pred_indices]\n",
    "    print('Log loss: {}'.format(log_loss(y, probas)))\n",
    "    print('Accuracy: {}'.format(accuracy_score(y, preds)))\n",
    "    skplt.metrics.plot_confusion_matrix(y, preds, figsize=(12,8))\n",
    "    skplt.metrics.plot_precision_recall_curve(y, probas, figsize=(12,8))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn_model = tf.keras.models.Sequential([\n",
    "#   tf.keras.layers.Dense(?, activation='relu'),\n",
    "#   tf.keras.layers.Dense(?, activation='relu'),\n",
    "#   tf.keras.layers.Dropout(0.2),\n",
    "#   tf.keras.layers.Dense(?number of types, activation='sigmoid')\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Keras tokenizer\n",
    "embed_dim = ?\n",
    "lstm_out = ?\n",
    "\n",
    "num_words = 2000\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(sample_train['Text'].values)\n",
    "\n",
    "# Pad the data \n",
    "X = tokenizer.texts_to_sequences(sample_train['Text'].values)\n",
    "X = pad_sequences(X, maxlen=2000)\n",
    "\n",
    "? output activation use softmax or sigmoid\n",
    "\n",
    "lstm_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Embedding(num_words, embed_dim),\n",
    "  tf.keras.layers.LSTM(lstm_out, recurrent_dropout=0.2, dropout=0.2),\n",
    "  tf.keras.layers.Dense(?,activation='sigmoid/softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = ?\n",
    "lstm_out = ?\n",
    "\n",
    "num_words = 2000\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(sample_train['Text'].values)\n",
    "\n",
    "# Pad the data \n",
    "X = tokenizer.texts_to_sequences(sample_train['Text'].values)\n",
    "X = pad_sequences(X, maxlen=2000)\n",
    "\n",
    "X_val = tokenizer.texts_to_sequences(sample_test['Text'].values)\n",
    "X_val = pad_sequences(X_val, maxlen=2000)\n",
    "\n",
    "? output activation use softmax or sigmoid\n",
    "\n",
    "bidirectional_lstm_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Embedding(num_words, embed_dim),\n",
    "  tf.keras.layers.Bidirectional(LSTM(64)),\n",
    "  tf.keras.layers.Dropout(0.5)\n",
    "  tf.keras.layers.Dense(?,activation='sigmoid/softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idf word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need pandas dataframe sample_train\n",
    "\n",
    "# count_vectorizer = TfidfVectorizer(\n",
    "#     analyzer=\"word\", tokenizer=nltk.word_tokenize,\n",
    "#     preprocessor=None, stop_words='english', max_features=None)    \n",
    "\n",
    "# tfidf = count_vectorizer.fit_transform(sample_train['text'])\n",
    "# tfidf_test = count_vectorizer.fit_transform(sample_test['text'])\n",
    "\n",
    "# len(count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd = TruncatedSVD(n_components=25, n_iter=25, random_state=12)\n",
    "# truncated_tfidf = svd.fit_transform(tfidf)\n",
    "# truncated_tfidf_test = svd.fit_transform(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the keras model\n",
    "# nn_model\n",
    "\n",
    "# X_train = truncated_tfidf\n",
    "# y_train = sample_train['type']\n",
    "\n",
    "# X_test = truncated_tfidf_test\n",
    "# y_test = sample_test['type']\n",
    "\n",
    "# nn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# nn_model.fit(X_train, y_train, epochs=300, batch_size=20, verbose=0)\n",
    "\n",
    "# _,train_accuracy = nn_model.evaluate(X_train, y_train, verbose=0)\n",
    "# print('Training Accuracy: %.2f' % (train_accuracy*100))\n",
    "\n",
    "# _,test_accuracy = nn_model.evaluate(X_test, y_test, verbose=0)\n",
    "# print('Test Accuracy: %.2f' % (test_accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_features(truncated_tfidf, sample_train['type'].values.ravel(), clf = nn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the keras model\n",
    "# lstm_model\n",
    "\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "lstm_model.fit(X, y_train, epochs=8, batch_size=20, validation_split=0.2, verbose=0)\n",
    "\n",
    "_,train_accuracy = lstm_model.evaluate(X, y_train, verbose=0)\n",
    "print('Training Accuracy: %.2f' % (train_accuracy*100))\n",
    "\n",
    "_,test_accuracy = lstm_model.evaluate(X_val, y_test, verbose=0)\n",
    "print('Test Accuracy: %.2f' % (test_accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the keras model\n",
    "# bidirectional_lstm_model\n",
    "\n",
    "bidirectional_lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "bidirectional_lstm_model.fit(X, y_train, epochs=8, batch_size=20, validation_split=0.2, verbose=0)\n",
    "\n",
    "_,train_accuracy = bidirectional_lstm_model.evaluate(X, y_train, verbose=0)\n",
    "print('Training Accuracy: %.2f' % (train_accuracy*100))\n",
    "\n",
    "_,test_accuracy = bidirectional_lstm_model.evaluate(X_val, y_test, verbose=0)\n",
    "print('Test Accuracy: %.2f' % (test_accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MySentences(object):\n",
    "\n",
    "#     def __init__(self, *arrays):\n",
    "#         self.arrays = arrays\n",
    " \n",
    "#     def __iter__(self):\n",
    "#         for array in self.arrays:\n",
    "#             for document in array:\n",
    "#                 for sent in nltk.sent_tokenize(document):\n",
    "#                     yield nltk.word_tokenize(sent)\n",
    "\n",
    "# def get_word2vec(sentences, location):\n",
    "\n",
    "#     if os.path.exists(location):\n",
    "#         print('Found {}'.format(location))\n",
    "#         model = gensim.models.Word2Vec.load(location)\n",
    "#         return model\n",
    "    \n",
    "#     print('{} not found. training model'.format(location))\n",
    "#     model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)\n",
    "#     print('Model done training. Saving to disk')\n",
    "#     model.save(location)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2vec = get_word2vec(\n",
    "#     MySentences(\n",
    "#         sample_train['text'].values, \n",
    "#     ),\n",
    "#     'w2vmodel'\n",
    "# )\n",
    "# w2vec_test = get_word2vec(\n",
    "#     MySentences(\n",
    "#         sample_test['text'].values, \n",
    "#     ),\n",
    "#     'w2vmodel_test'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyTokenizer:\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "    \n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "    \n",
    "#     def transform(self, X):\n",
    "#         transformed_X = []\n",
    "#         for document in X:\n",
    "#             tokenized_doc = []\n",
    "#             for sent in nltk.sent_tokenize(document):\n",
    "#                 tokenized_doc += nltk.word_tokenize(sent)\n",
    "#             transformed_X.append(np.array(tokenized_doc))\n",
    "#         return np.array(transformed_X)\n",
    "    \n",
    "#     def fit_transform(self, X, y=None):\n",
    "#         return self.transform(X)\n",
    "\n",
    "# class MeanEmbeddingVectorizer(object):\n",
    "#     def __init__(self, word2vec):\n",
    "#         self.word2vec = word2vec\n",
    "#         self.dim = len(word2vec.wv.syn0[0])\n",
    "\n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         X = MyTokenizer().fit_transform(X)\n",
    "        \n",
    "#         return np.array([\n",
    "#             np.mean([self.word2vec.wv[w] for w in words if w in self.word2vec.wv]\n",
    "#                     or [np.zeros(self.dim)], axis=0)\n",
    "#             for words in X\n",
    "#         ])\n",
    "    \n",
    "#     def fit_transform(self, X, y=None):\n",
    "#         return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_embedding_vectorizer = MeanEmbeddingVectorizer(w2vec)\n",
    "# mean_embedding_vectorizer_test = MeanEmbeddingVectorizer(w2vec_test)\n",
    "\n",
    "# mean_embedded = mean_embedding_vectorizer.fit_transform(sample_train['text'])\n",
    "# mean_embedded_test = mean_embedding_vectorizer_test.fit_transform(sample_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the keras model\n",
    "# nn_model\n",
    "\n",
    "# X_train = mean_embedded\n",
    "# y_train = sample_train['type']\n",
    "\n",
    "# X_test = mean_embedded_test\n",
    "# y_test = sample_test['type']\n",
    "\n",
    "# nn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# nn_model.fit(X_train, y_train, epochs=300, batch_size=20, verbose=0)\n",
    "\n",
    "# _,train_accuracy = nn_model.evaluate(X_train, y_train, verbose=0)\n",
    "# print('Training Accuracy: %.2f' % (train_accuracy*100))\n",
    "\n",
    "# _,test_accuracy = nn_model.evaluate(X_test, y_test, verbose=0)\n",
    "# print('Test Accuracy: %.2f' % (test_accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_features(mean_embedded, sample_train['type'].values.ravel(), clf = nn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need selected word embedding and model\n",
    "\n",
    "selected_model = ?\n",
    "\n",
    "# probas = cross_val_predict(selected_model, example_X, example_y, cv=StratifiedKFold(random_state=8), \n",
    "#                               n_jobs=-1, method='predict_proba', verbose=2)\n",
    "\n",
    "probas = selected_model.predict_proba(example_X)\n",
    "pred_indices = np.argmax(probas, axis=1)\n",
    "\n",
    "# here classes needed\n",
    "preds = classes[pred_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity Linking and Typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(word):\n",
    "    return\n",
    "\n",
    "def get_number_links(entity, type):\n",
    "    return\n",
    "\n",
    "def prob_entity_link_to_type(entities_of_word, word_type):\n",
    "    links = []\n",
    "    for entity in entities_of_word:\n",
    "        links.append(get_number_links(entity, word_type))\n",
    "    links = np.array(links)\n",
    "    return links/np.sum(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_sentence = []\n",
    "\n",
    "for idx, word in enumerate(sentence):\n",
    "    word_type = \n",
    "    entities_of_word = get_entities(word)\n",
    "    prob = probas[pred_indices[idx]]*prob_entity_link_to_type(entities_of_word, word_type)\n",
    "    entity_sentence.append(entities_of_word(prob.index(max(prob))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_model = ?\n",
    "\n",
    "new_sentence = []\n",
    "for idx, word in enumerate(sentence):\n",
    "    word_type = preds[idx]\n",
    "    new_sentence.append(word_type)\n",
    "\n",
    "def predict_entity(sentence):\n",
    "    entities = []\n",
    "    for idx in range(len(sentence)):\n",
    "        word = sentence[idx]\n",
    "        new_sentence = []\n",
    "        for i in range(len(sentence)):\n",
    "            if i != idx:\n",
    "                word_type = preds[idx]\n",
    "                new_sentence.append(word_type)\n",
    "            else:\n",
    "                new_sentence.append(sentence[idx])\n",
    "        new_probas = selected_model.predict_proba(new_sentence)\n",
    "        new_prob = new_probas[idx]\n",
    "        new_prob_idx = new_prob.index(max(new_prob))\n",
    "        \n",
    "        word_type = preds[new_prob_idx]\n",
    "        entities_of_word = get_entities(word)\n",
    "        prob = new_prob[new_prob_idx]*prob_entity_link_to_type(entities_of_word, word_type)\n",
    "        entities.append(entities_of_word(prob.index(max(prob))))\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
